# HBase / HDFS - Chi Ti·∫øt Tri·ªÉn Khai

## üéØ M·ª•c Ti√™u

L∆∞u tr·ªØ d·ªØ li·ªáu raw v√† l·ªãch s·ª≠ d√†i h·∫°n, ph·ª•c v·ª• cho ph√¢n t√≠ch batch v√† truy v·∫•n nhanh.

## üìä Ki·∫øn Tr√∫c L∆∞u Tr·ªØ

### HBase: L∆∞u Tr·ªØ D·ªØ Li·ªáu B√°n C·∫•u Tr√∫c

#### 1. Table: traffic_violations
- **M·ª•c ƒë√≠ch**: L∆∞u b·∫£n tin vi ph·∫°m g·ªëc
- **Rowkey Design**: `{region}#{timestamp}#{camera_id}#{license_plate}`
  - VD: `BDG#1719448991037#BDG_CAM_001#11H03226`
  - ƒê·∫£m b·∫£o ph√¢n b·ªë ƒë·ªÅu theo region
  - D·ªÖ query theo th·ªùi gian v√† region

#### 2. Table: traffic_metrics
- **M·ª•c ƒë√≠ch**: L∆∞u metrics ƒë√£ aggregate
- **Rowkey Design**: `{window_type}#{window_start}#{region}`
  - VD: `5min#2024-06-27T10:00:00#BDG`
  - Window types: `5min`, `15min`, `1hour`, `1day`

#### 3. Table: traffic_hotspots
- **M·ª•c ƒë√≠ch**: L∆∞u l·ªãch s·ª≠ hotspot detection
- **Rowkey Design**: `{date}#{hour}#{camera_id}`
  - VD: `2024-06-27#10#BDG_CAM_001`

### HDFS: L∆∞u Tr·ªØ File Parquet

#### 1. Raw Data (Parquet)
- **Path**: `/data/traffic_violations/raw/year={year}/month={month}/day={day}/`
- **Format**: Parquet
- **Partition**: Theo ng√†y
- **Schema**: Gi·ªëng schema Avro trong NiFi

#### 2. Processed Data (Parquet)
- **Path**: `/data/traffic_violations/processed/year={year}/month={month}/day={day}/`
- **Format**: Parquet
- **Content**: D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† enrich

#### 3. Metrics Data (Parquet)
- **Path**: `/data/traffic_metrics/{window_type}/year={year}/month={month}/day={day}/`
- **Format**: Parquet
- **Window types**: `5min`, `15min`, `1hour`, `1day`

## üîß Thi·∫øt K·∫ø HBase Tables

### 1. Table: traffic_violations

```bash
# T·∫°o table
create 'traffic_violations', 
  {NAME => 'info', VERSIONS => 1, COMPRESSION => 'SNAPPY'},
  {NAME => 'media', VERSIONS => 1, COMPRESSION => 'SNAPPY'},
  {NAME => 'metadata', VERSIONS => 1, COMPRESSION => 'SNAPPY'}

# Column Families:
# - info: Th√¥ng tin c∆° b·∫£n (camera_id, violation_type, license_plate, etc.)
# - media: ƒê∆∞·ªùng d·∫´n media (video_path, image_paths)
# - metadata: Metadata (ingest_timestamp, source_file, region)
```

**Column Design**:
- `info:camera_id`: Camera ID
- `info:vin`: VIN
- `info:timestamp`: Timestamp (milliseconds)
- `info:timestamp_iso`: ISO 8601 timestamp
- `info:violation_type`: Lo·∫°i vi ph·∫°m
- `info:license_plate`: Bi·ªÉn s·ªë xe
- `info:processing_status`: Tr·∫°ng th√°i x·ª≠ l√Ω
- `info:violation_severity`: M·ª©c ƒë·ªô vi ph·∫°m
- `info:vehicle_color`: M√†u xe
- `info:plate_color`: M√†u bi·ªÉn s·ªë
- `media:video_path`: ƒê∆∞·ªùng d·∫´n video
- `media:plate_image`: ƒê∆∞·ªùng d·∫´n ·∫£nh bi·ªÉn s·ªë
- `media:overview_image`: ƒê∆∞·ªùng d·∫´n ·∫£nh t·ªïng quan
- `media:vehicle_image`: ƒê∆∞·ªùng d·∫´n ·∫£nh xe
- `media:before_image`: ƒê∆∞·ªùng d·∫´n ·∫£nh tr∆∞·ªõc
- `media:additional_image`: ƒê∆∞·ªùng d·∫´n ·∫£nh b·ªï sung
- `metadata:region`: Region code
- `metadata:ingest_timestamp`: Th·ªùi gian ingest
- `metadata:source_file`: File ngu·ªìn

### 2. Table: traffic_metrics

```bash
create 'traffic_metrics',
  {NAME => 'metrics', VERSIONS => 1, COMPRESSION => 'SNAPPY'},
  {NAME => 'details', VERSIONS => 1, COMPRESSION => 'SNAPPY'}
```

**Column Design**:
- `metrics:window_type`: Lo·∫°i window (5min, 15min, 1hour)
- `metrics:window_start`: Th·ªùi gian b·∫Øt ƒë·∫ßu window
- `metrics:window_end`: Th·ªùi gian k·∫øt th√∫c window
- `metrics:violation_count`: T·ªïng s·ªë vi ph·∫°m
- `metrics:unique_vehicles`: S·ªë ph∆∞∆°ng ti·ªán duy nh·∫•t
- `metrics:traffic_volume`: L∆∞u l∆∞·ª£ng giao th√¥ng
- `metrics:violation_rate`: T·ª∑ l·ªá vi ph·∫°m
- `details:cameras`: Danh s√°ch camera (JSON)
- `details:violation_types`: Ph√¢n b·ªë lo·∫°i vi ph·∫°m (JSON)

### 3. Table: traffic_hotspots

```bash
create 'traffic_hotspots',
  {NAME => 'hotspot', VERSIONS => 1, COMPRESSION => 'SNAPPY'}
```

**Column Design**:
- `hotspot:camera_id`: Camera ID
- `hotspot:region`: Region
- `hotspot:violation_count`: S·ªë vi ph·∫°m
- `hotspot:threshold`: Ng∆∞·ª°ng
- `hotspot:is_active`: Tr·∫°ng th√°i active

## üìù Ghi D·ªØ Li·ªáu V√†o HBase

### T·ª´ Spark Streaming

```python
from pyspark.sql import SparkSession
import happybase

def write_to_hbase(batch_df, batch_id):
    connection = happybase.Connection('localhost')
    table = connection.table('traffic_violations')
    
    for row in batch_df.collect():
        rowkey = f"{row.region}#{row.timestamp}#{row.camera_id}#{row.license_plate}"
        
        data = {
            b'info:camera_id': str(row.camera_id).encode(),
            b'info:timestamp': str(row.timestamp).encode(),
            b'info:violation_type': str(row.violation_type).encode(),
            b'info:license_plate': str(row.license_plate).encode(),
            b'info:vehicle_color': str(row.vehicle_color or '').encode(),
            b'info:plate_color': str(row.plate_color or '').encode(),
            b'media:video_path': str(row.video_path or '').encode(),
            b'media:plate_image': str(row.plate_image_path or '').encode(),
            b'metadata:region': str(row.region).encode(),
            b'metadata:ingest_timestamp': str(row.ingest_timestamp).encode()
        }
        
        table.put(rowkey, data)
    
    connection.close()

# S·ª≠ d·ª•ng trong Spark Streaming
violations_df.writeStream \
    .foreachBatch(write_to_hbase) \
    .option("checkpointLocation", "/checkpoint/hbase") \
    .start()
```

### T·ª´ Spark Batch (HDFS ‚Üí HBase)

```python
# ƒê·ªçc t·ª´ HDFS Parquet
df = spark.read.parquet("/data/traffic_violations/raw/year=2024/month=06/day=27/")

# Transform v√† ghi v√†o HBase
df.foreachPartition(write_partition_to_hbase)
```

## üìÅ C·∫•u Tr√∫c HDFS

```
/data/
‚îú‚îÄ‚îÄ traffic_violations/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ year=2024/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=06/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=27/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ part-00000.parquet
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ part-00001.parquet
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ day=28/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ month=07/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ year=2024/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ month=06/
‚îÇ   ‚îÇ               ‚îî‚îÄ‚îÄ day=27/
‚îÇ   ‚îî‚îÄ‚îÄ metrics/
‚îÇ       ‚îú‚îÄ‚îÄ 5min/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ year=2024/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ month=06/
‚îÇ       ‚îÇ           ‚îî‚îÄ‚îÄ day=27/
‚îÇ       ‚îú‚îÄ‚îÄ 15min/
‚îÇ       ‚îú‚îÄ‚îÄ 1hour/
‚îÇ       ‚îî‚îÄ‚îÄ 1day/
‚îî‚îÄ‚îÄ traffic_volume/
    ‚îî‚îÄ‚îÄ year=2024/
        ‚îî‚îÄ‚îÄ month=06/
            ‚îî‚îÄ‚îÄ day=27/
```

## üîÑ Ghi D·ªØ Li·ªáu V√†o HDFS

### T·ª´ Spark Streaming

```python
# Ghi Parquet v√†o HDFS
query_hdfs = violations_df \
    .writeStream \
    .format("parquet") \
    .option("path", "/data/traffic_violations/raw") \
    .option("checkpointLocation", "/checkpoint/hdfs") \
    .partitionBy("year", "month", "day") \
    .start()
```

### T·ª´ Spark Batch

```python
# ƒê·ªçc t·ª´ Kafka ho·∫∑c HBase
df = spark.read.format("kafka")...

# Ghi v√†o HDFS
df.write \
    .mode("append") \
    .partitionBy("year", "month", "day") \
    .parquet("/data/traffic_violations/raw")
```

## üîç Query HBase

### 1. Query Theo Region v√† Th·ªùi Gian

```python
import happybase

connection = happybase.Connection('localhost')
table = connection.table('traffic_violations')

# Scan theo region v√† timestamp range
start_row = f"BDG#1719448990000"
stop_row = f"BDG#1719449000000"

for key, data in table.scan(row_start=start_row, row_stop=stop_row):
    print(key, data)
```

### 2. Query Metrics

```python
# Query metrics 5 ph√∫t
table = connection.table('traffic_metrics')

start_row = "5min#2024-06-27T10:00:00"
stop_row = "5min#2024-06-27T11:00:00"

for key, data in table.scan(row_start=start_row, row_stop=stop_row):
    print(key, data)
```

## üìä T·ªëi ∆Øu H√≥a

### 1. Pre-splitting Regions

```bash
# Pre-split table theo region
create 'traffic_violations', 
  {NAME => 'info', VERSIONS => 1, COMPRESSION => 'SNAPPY'},
  {NAME => 'media', VERSIONS => 1, COMPRESSION => 'SNAPPY'},
  {NAME => 'metadata', VERSIONS => 1, COMPRESSION => 'SNAPPY'},
  {SPLITS => ['BDG', 'DTP', 'TVH', 'BKN']}
```

### 2. Compression
- S·ª≠ d·ª•ng `SNAPPY` cho t·ªëc ƒë·ªô ƒë·ªçc/ghi t·ªët
- Ho·∫∑c `GZ` cho t·ª∑ l·ªá n√©n cao h∆°n

### 3. Bloom Filter
- Enable bloom filter cho column `info:license_plate` ƒë·ªÉ tƒÉng t·ªëc query

```bash
alter 'traffic_violations', {NAME => 'info', BLOOMFILTER => 'ROWCOL'}
```

## üéØ K·∫øt Qu·∫£

- **Scalable Storage**: L∆∞u tr·ªØ h√†ng tri·ªáu b·∫£n ghi
- **Fast Query**: Query nhanh v·ªõi rowkey design h·ª£p l√Ω
- **Data Lake**: HDFS l√†m data lake cho ph√¢n t√≠ch batch
- **Historical Data**: L∆∞u tr·ªØ l·ªãch s·ª≠ d√†i h·∫°n

## üìå T·ª´ Kh√≥a B·∫£o V·ªá

- **Distributed Storage**: L∆∞u tr·ªØ ph√¢n t√°n
- **NoSQL Database**: HBase cho d·ªØ li·ªáu b√°n c·∫•u tr√∫c
- **Data Lake**: HDFS l√†m data lake
- **Rowkey Design**: Thi·∫øt k·∫ø rowkey t·ªëi ∆∞u
- **Column Families**: T·ªï ch·ª©c d·ªØ li·ªáu theo column families

